{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluedo Client\n",
    "This script demonstrates the workflow of a Retrieval-Augmented Generation (RAG) architecture. A Pinecone vector database has been set up and populated with data in a separate script (cluedo_vdb_setup.ipynb).\n",
    "\n",
    "The use case of this project is a murder mystery scenario inspired by the board game Cluedo. Users can submit questions about the characters, the events of the case, and—most importantly—the identity of the murderer.\n",
    "\n",
    "The user’s query is first used to retrieve relevant contextual information from the vector database. This retrieved context is appended to the original question, forming an enhanced prompt that is then submitted to a large language model (LLM). If the provided context is insufficient to answer the question, the LLM is explicitly instructed to request additional information from the database.\n",
    "\n",
    "For this project, the GPT-OSS-120B model with 120 billion parameters was selected. The model is accessed via the Inference API provided by Hugging Face (see https://huggingface.co/openai/gpt-oss-120b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import textwrap\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import InferenceClient\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = yaml.load(open(\"conf.yml\", \"r\"), Loader=yaml.SafeLoader)\n",
    "\n",
    "# credentials and configuration\n",
    "hf_token = conf[\"hugging_face\"][\"token\"]\n",
    "pc_token = conf[\"pinecone\"][\"token\"]\n",
    "pc_index_name = \"cluedo-py\"\n",
    "llm_name = \"openai/gpt-oss-120b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve context from vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_client = Pinecone(api_key=pc_token)\n",
    "\n",
    "# retrieve context from the vector db based on a query parameter\n",
    "def retrieve_context(query, context_size, verbose=False):\n",
    "    index = pc_client.Index(pc_index_name)\n",
    "\n",
    "    # search the dense index and rerank results\n",
    "    reranked_results = index.search(\n",
    "        namespace=\"__default__\",\n",
    "        query={\n",
    "            \"top_k\": context_size,\n",
    "            \"inputs\": {\n",
    "                'text': query\n",
    "            }\n",
    "        },\n",
    "        rerank={\n",
    "            \"model\": \"bge-reranker-v2-m3\",\n",
    "            \"top_n\": context_size,\n",
    "            \"rank_fields\": [\"chunk_text\"]\n",
    "        } \n",
    "    )\n",
    "\n",
    "    # join the context entries to create one string\n",
    "    if verbose:\n",
    "        print(\"Retrieved context: \")\n",
    "    \n",
    "    context = \"\"\n",
    "    for hit in reranked_results['result']['hits']:\n",
    "        context += hit['fields']['chunk_text'] + \" \"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"id: {hit['_id']:<5} | score: {round(hit['_score'], 2):<5} | text: {hit['fields']['chunk_text']:<50}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"---------\")\n",
    "\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send query to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = InferenceClient(token=hf_token)\n",
    "\n",
    "# send a query to the llm, return response\n",
    "def submit_to_llm(query):\n",
    "    instructions = \"\"\"\n",
    "        Analyze the contextual information about an old estate and the people living and working there. \n",
    "        There is more contextual information, that you can ask about.\n",
    "        If you don't have enough contextual information to answer the question with high certainty, write \"Need more information about: X\" \n",
    "        and specify X as one specific term mentioned in the concept. \n",
    "        Do not ask about general concepts like \"motive\" or \"alibi\". \n",
    "        If you have enough contextual information to answer the question with high certainty, answer the question in one paragraph and argue your reasoning.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=llm_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": instructions + query }]\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_cluedo_rag(query):\n",
    "    curr_query = query\n",
    "    context = \"\"\n",
    "    answer = \"\"\n",
    "    context_size = 10\n",
    "    retrieval_count = 0\n",
    "\n",
    "    while len(answer) == 0 or str.startswith(answer, \"Need more information about: \") and retrieval_count < 5:\n",
    "        # retrieve context to question\n",
    "        context += retrieve_context(curr_query, context_size, verbose=False)\n",
    "        enhanced_query = \" \".join([context, query])\n",
    "        retrieval_count += 1\n",
    "\n",
    "        # ask llm the question + context\n",
    "        response = submit_to_llm(enhanced_query)\n",
    "\n",
    "        # output the llm's answer\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # if the llm needs more information, update query and repeat\n",
    "        if str.startswith(answer, \"Need more information about: \"):\n",
    "            curr_query = str.removeprefix(answer, \"Need more information about: \")\n",
    "            print(\"Retrieving new context about \"+curr_query)\n",
    "\n",
    "    # Either print the answer or state that there is not enough information in the database\n",
    "    if str.startswith(answer, \"Need more information about: \"):\n",
    "        print(\"There is not enough information to answer this question.\")\n",
    "    else:\n",
    "        if retrieval_count > 1:\n",
    "            print(\"---------\")\n",
    "        print(textwrap.fill(answer, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estate comprises several distinct locations, each with its own function: the **guesthouse**, where overnight\n",
      "visitors are accommodated; the **garage**, used for storing and maintaining the lord’s vehicles; the **basement**, a\n",
      "lower‑level space that likely houses utilities, storage, or service areas; the **large garden**, tended by the gardener\n",
      "and providing outdoor space for recreation, landscaping, and possibly food production; and the **shed**, a restricted\n",
      "area that can only be accessed by the gardener, the butler, and the lord, where tools, equipment, or supplies are kept.\n",
      "The **lord** oversees the property, the **maid** works the household (though her specific duties aren’t detailed), the\n",
      "**supplier** delivers food weekly to the estate (presumably to the kitchen or pantry), and the **gardener** maintains\n",
      "the garden and accesses the shed for maintenance tasks.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which locations does the estate have and what happens in these locations?\"\n",
    "\n",
    "ask_cluedo_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The people who both live and work on the estate, based on the provided details, are the lord (the owner and resident),\n",
      "the maid (the newest employee), the gardener (who has shed access and drives to work), the butler (who also has shed\n",
      "access), the chef (who drives to work with the gardener), and the supplier (who delivers food weekly). These six\n",
      "individuals are explicitly mentioned as either staff members or the proprietor who are present on‑site for work or\n",
      "residence.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which characters live and work around the estate?\"\n",
    "\n",
    "ask_cluedo_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving new context about drink.\n",
      "---------\n",
      "The residents’ and staff’s food and drink preferences can be deduced from the details given: the lord’s favourite meals\n",
      "are meat‑based, so he regularly eats meat and, although his specific beverage isn’t stated, his gambling habit and the\n",
      "household’s access to alcohol (via the chef) suggest he likely drinks alcohol as well; the lady is a vegetarian, so she\n",
      "eats plant‑based dishes and would avoid meat; the chef’s alcohol problem indicates he himself enjoys drinking alcohol;\n",
      "the chauffeur is a recovering alcoholic, so he deliberately refrains from alcohol; the gardener, maid and supplier have\n",
      "no explicit preferences mentioned, so their eating and drinking habits remain unspecified.\n"
     ]
    }
   ],
   "source": [
    "question = \"What do people at the estate like to eat and drink?\"\n",
    "\n",
    "ask_cluedo_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving new context about discipline.\n",
      "Retrieving new context about punctuality\n",
      "---------\n",
      "The lord’s staff show a mixed level of discipline: the chauffeur appears reliable—he consistently cares for the lord’s\n",
      "yellow Peugeot and drives him to appointments—while the butler enforces strict rules, constantly reprimanding the\n",
      "gardener for taking “too many breaks,” indicating his own disciplined attitude. By contrast, the gardener’s absenteeism\n",
      "(he is sick and missed work) and the chef’s known alcohol problem, plus his illegal driving (he has no licence yet still\n",
      "drives to work with the gardener), reveal considerable lapses in personal discipline. The maid, supplier and gardener’s\n",
      "routine duties are not described as problematic, suggesting they are at least nominally compliant. Overall, some\n",
      "employees (the chauffeur and butler) are disciplined, but others (the chef and gardener) exhibit notable indiscipline.\n"
     ]
    }
   ],
   "source": [
    "question = \"How disciplined are the employees of the lord?\"\n",
    "\n",
    "ask_cluedo_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving new context about murderer.\n",
      "---------\n",
      "The guest—an employee of a bank who had been invited to the estate by the lord—was found dead in the garden, having been\n",
      "bludgeoned to death with a shovel. The shovel is most plausibly one of the tools kept in the garden shed, which the\n",
      "gardener routinely uses while working the grounds; the gardener is also the cousin of the chef, giving him easy access\n",
      "to the weapon and the crime scene. Adding motive, the guest was carrying on an affair with the 25‑year‑old maid (the\n",
      "butler’s niece and the estate’s newest employee), a relationship that could have provoked jealousy or a lethal\n",
      "confrontation, while the lady of the house, a vegetarian, is in a personal feud with the chef (who is employed by the\n",
      "lord), suggesting the chef might also have a grievance but no direct link to the shovel. Thus, the confirmed facts of\n",
      "the case are: victim = bank employee guest; weapon = shovel from the garden shed; location = garden; relationship ties =\n",
      "affair with the maid, feud between lady and chef, familial link between gardener and chef, and the butler’s connection\n",
      "to the maid. These details establish the who, what, where, and how of the murder, while indicating the gardener (with\n",
      "tool access) and the maid (with intimate motive) as the primary persons of interest.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the details of the murder case?\"\n",
    "\n",
    "ask_cluedo_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The strategy of prompting the model to explicitly request additional information on a given topic performs surprisingly well in certain cases. Although it remains unclear under which conditions the LLM decides to request further information, the generated queries—when issued—are often highly relevant and well-suited for retrieval from the database. Future experiments using a larger dataset could provide valuable insights into the impact of different context-window sizes. At present, the context window is limited to 10 entries. A window that is too small may exclude relevant information (e.g., when querying \"Who works at the estate?\", the number of employees retrievable is constrained by the context-window size), whereas an excessively large window may introduce irrelevant information and thereby reduce model performance.\n",
    "\n",
    "Further investigation should also consider the size of the total context. As the model requests additional information across multiple topics, newly retrieved content is appended to the existing context, leading to progressively expanding context windows. In the current implementation, the number of information requests is capped at five to prevent overloading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluedoRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
